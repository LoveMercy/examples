
####  Language model和Sequence2Sequence的目标函数&emsp; &ensp;
- 联合概率与条件概率：
```math
p(x,y) = p(x|y)p(y) = p(y|x)p(x)
```
当`$x$`和`$y$`相互独立时有：
```math
p(x,y) = p(x)p(y),
p(x|y) = p(x),
p(y|x) = p(y)
```
- 当`$x$`和`$y$`不相互独立时有：
 ```math
p(x,y) = p(x|y)p(y) \ge p(x)p(y)
```
- 贝叶斯公式：

```math
p(x|y) = \frac{{p(x,y)}}{{p(y)}} = \frac{{p(y|x)p(x)}}{{p(y)}}
```
```math
p(y|x) = \frac{{p(x,y)}}{{p(x)}} = \frac{{p(x|y)p(y)}}{{p(x)}}
``` 
- `$PMI$`点互信息：
 
```math
PMI(x,y) = \frac{{p(x,y)}}{{p(x)p(y)}} = \frac{{p(x|y)p(y)}}{{p(x)p(y)}} = \frac{{p(x|y)}}{{p(x)}} = \frac{{p(y|x)}}{{p(y)}}
```

- 似然函数：likelihood

    &emsp;似然就是概率，似然函数的意思是给定输入样本下，输出为目标样本的概率大小：`$L = p(y|x,\theta )$`，
    `$\theta$`为模型参数。
- 极大似然估计：maximum likelihood estimation
    
    就是让似然函数最大`$\mathop {\arg \max }\limits_\theta  (L = p(y|x,\theta ))$` 后取其参数 `$\theta$` 作为最优参数。


训练样本中有多个样本对`$\{ \overrightarrow {{x_1}} :\overrightarrow {{y_1}} ,\overrightarrow {{x_2}} :\overrightarrow {{y_2}} , \cdots ,\overrightarrow {{x_n}} :\overrightarrow {{y_n}} \} $` ，对于其中某个样本对 `$\overrightarrow x :\overrightarrow y $`，给定输入样本 `$\overrightarrow x  = ({x_1},{x_2}, \cdots ,{x_{{n_x}}})$`，输出样本`$\overrightarrow y  = ({y_1},{y_2}, \cdots ,{y_{{n_y}}})$` ，`${n_x}$` 为输入样本的个数，`${n_y}$` 为输出样本的个数， `$V$`为词表的大小。

- Language model：
语言模型为给定前几个词来预测后面的词的概率。

- 目标函数：

    ```math
    p(\overrightarrow y ) = \prod\limits_{t = 1}^{{n_y}} {p({y_t}|{y_1},{y_2}, \cdots ,{y_{t - 1}})}
    ```
    ，使其最大。

- Sequence2Sequence model目标函数：

    ```math
    p(\overrightarrow y |\overrightarrow x ) = \prod\limits_{t = 1}^{{n_y}} {p({y_t}|{y_1},{y_2}, \cdots ,{y_{t - 1}},\overrightarrow x )} 
    ```
     
    ，使其最大。

- Training：

    最小化负对数似然。
    Given a training dataset where each target y is paired with a source x, the learning objective
    is to minimize the negative log-likelihood of predicting each word in the target y given the
    source x:
 
    ```math
    J =  - \sum\limits_{t = 1}^{{n_y}} {\log p({y_t}|{y_1},{y_2}, \cdots ,{y_{t - 1}},\overrightarrow x )} 
    ```

- 交叉熵：
	  
    ```math
    H(p,q) =  - \sum\limits_i {p(i)\log q(i)}
    ```
    
    衡量两个分布之间的差异性。

- 分类问题中极大似然估计和交叉熵的一致性：
	
    在实际的操作中，都用交叉熵目标函数来代替似然目标函数，两者在分类问题上是一致的。

- 极大似然估计目标函数：
    	
    对于某一个词`${y_t}$`的生成，极大似然估计的目标函数为：
    ```math
    J =  - \log p({y_t}|{y_1},{y_2}, \cdots ,{y_{t - 1}},\overrightarrow x )
    ```	   
    
    ……………………………………….eq1
   
    将`${y_1},{y_2}, \cdots ,{y_{t - 1}},\overrightarrow x $` 输入，目的是让词`${y_t}$` 的概率最大。

- 交叉熵目标函数：

    将标签用one-hot表示：
    
    ```math
    \mathop l\limits^\  = ({y_1} = 0,{y_2} = 0, \cdots ,{y_t} = 1, \cdots ,{y_v} = 0)
    ```
    ，只有 那一项的概率为1，其余都为0.
    
    真实输出为：
    
    ```math
    l = ({y_1} = p({y_1}|...),{y_2} = p({y_2}|...), \cdots ,{y_t} = p({y_t}|...), \cdots ,{y_v} = p({y_v}|...))
    ```

    计算两个分布的交叉熵：

    ```math
    H(l,\mathop l\limits^\ ) =  - \sum\limits_{i = 1}^v {\mathop l\limits^\ (i)\log l(i)}  =  - \log p({y_t}|...)
    ```

    ………………………………eq2
    
    Eq1和eq2两个目标函数相同，在实际操作中用的是交叉熵目标函数。
    当目标是一个句子时，具体做法为先计算每个词的交叉熵损失，再把这个句子中所有词的交叉熵损失都计算出来然后加和作为最终的损失函数。


